{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26a121c",
   "metadata": {},
   "source": [
    "# Data Collection Phase\n",
    "\n",
    "We have to scrape at least 20000 rows of data. We can scrape more data as well, itâ€™s up to you. more the data better the model\n",
    "In this section we need to scrape the reviews of different laptops, Phones, Headphones, smart watches, Professional Cameras, Printers, Monitors, Home theater, Router from different e-commerce websites.\n",
    "\n",
    "Basically, we need these columns:\n",
    "1. reviews of the product.\n",
    "2. rating of the product.\n",
    "\n",
    "We can fetch other data as well, if you think data can be useful or can help in the project. It completely depends on your imagination or assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ecd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the neccessary libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "import  requests\n",
    "from  bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dd857",
   "metadata": {},
   "source": [
    "### Reviews from Flipkart.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ff344",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\web driver\\chromedriver.exe\")\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "#clicking on cancel button \n",
    "driver.find_element_by_xpath(\"//div[@class='_2QfC02']/button\").click()\n",
    "srch_items = ['laptops', 'Phones','smart watches','Monitors']\n",
    "title = []\n",
    "review_text = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap():    \n",
    "        for i in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "            review_text.append(i.text)\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "            title.append(i.text)\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']/../div\"):\n",
    "            ratings.append(i.text)\n",
    "        return\n",
    "urls=[]\n",
    "for i in srch_items:\n",
    "    # Find the search bar\n",
    "    srchBar = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input\")\n",
    "    srchBar.clear()\n",
    "    srchBar.send_keys(i)\n",
    "    # Clicking on the search button\n",
    "    driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "    time.sleep(3)\n",
    "    page = []\n",
    "    for i in driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\"):\n",
    "        page.append(i.get_attribute('href'))\n",
    "    for i in page[0:4]:\n",
    "        driver.get(i)\n",
    "        time.sleep(3)\n",
    "        items = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "        for i in items:\n",
    "            urls.append(i.get_attribute('href'))\n",
    "len(urls)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635220bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    for _ in range(2):\n",
    "        driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "        time.sleep(1)\n",
    "    # Clicking on all reviews\n",
    "    try:\n",
    "        btn=driver.find_element_by_xpath(\"//div[@class='_2c2kV-']/following::a\")\n",
    "        lnk = btn.get_attribute('href')\n",
    "        driver.get(lnk)\n",
    "        time.sleep(1)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    scrap()        \n",
    "    try:\n",
    "        n_page=driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\")\n",
    "        np=[]\n",
    "        for i in n_page:\n",
    "            np.append(i.get_attribute('href'))\n",
    "        for i in np[0:18]:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            scrap()\n",
    "    except: continue\n",
    "len(ratings), len(review_text), len(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c98db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe\n",
    "data = list(zip(title,review_text,ratings))\n",
    "df = pd.DataFrame(data, columns = [\"Review_title\",\"Review_text\",\"Ratings\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data into csv file\n",
    "df.to_csv(r\"C:\\Users\\Vishwas Pai\\flipkart1_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c5377",
   "metadata": {},
   "source": [
    "### Reviews from Amazon.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32087c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "srch_items = ['laptops','phones','headphones','smart watches', 'Professional Cameras', 'Printers', \n",
    "              'Monitors', 'Home theater', 'Router']\n",
    "title = []\n",
    "review_text = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in srch_items:\n",
    "    srchbar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    srchbar.send_keys(i)\n",
    "    driver.find_element_by_id(\"nav-search-submit-button\").click()  #clicking on search button\n",
    "    time.sleep(1)\n",
    "    item_url = []\n",
    "    for i in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        item_url.append(i.get_attribute('href'))\n",
    "    for i in item_url:\n",
    "        driver.get(i)\n",
    "        time.sleep(1)\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"window.scrollBy(0,6000)\")\n",
    "            time.sleep(1)\n",
    "        # Clicking on see all reviews\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//a[@class='a-link-emphasis a-text-bold']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        url_ = []\n",
    "        try:\n",
    "            two_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[4]/td[2]/a\")\n",
    "            url_.append(two_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            three_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[3]/td[2]/a\")\n",
    "            url_.append(three_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            one_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[5]/td[2]/a\")\n",
    "            url_.append(one_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            fiv_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[1]/td[2]/a\")\n",
    "            url_.append(fiv_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        try:\n",
    "            four_str = driver.find_element_by_xpath(\"//table[@id='histogramTable']/tbody/tr[2]/td[2]/a\")\n",
    "            url_.append(four_str.get_attribute('href'))\n",
    "        except: pass\n",
    "        \n",
    "        for i in url_:\n",
    "            driver.get(i)\n",
    "            time.sleep(1)\n",
    "            for i in range(10):\n",
    "                ids = driver.find_elements_by_xpath(\"//div[@class='a-section a-spacing-none review-views celwidget']/div\")\n",
    "                comment_ids = []\n",
    "                for i in ids:\n",
    "                    comment_ids.append(i.get_attribute('id'))\n",
    "\n",
    "                for x in comment_ids:\n",
    "                    # Extract user ids from each user on a page\n",
    "                    try:\n",
    "                        review_title = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[2]/a[2]/span[1]')\n",
    "                        title.append(review_title.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        title.append('')\n",
    "\n",
    "                    try:\n",
    "                        rating = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[2]/a[1]/i/span[1]')\n",
    "                        ratings.append(rating.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        ratings.append('')\n",
    "\n",
    "                    try:\n",
    "                        text = driver.find_element_by_xpath('//*[@id=\"' + x +'\"]/div/div/div[4]/span/span')\n",
    "                        review_text.append(text.get_attribute(\"innerHTML\"))\n",
    "                    except NoSuchElementException:\n",
    "                        review_text.append('')\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//ul[@class='a-pagination']/li[2]/a\").click()\n",
    "                    time.sleep(3)\n",
    "                except : break \n",
    "len(ratings), len(title), len(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe\n",
    "data = list(zip(title,review_text,ratings))\n",
    "df = pd.DataFrame(data, columns = [\"Review_title\",\"Review_text\",\"Ratings\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data into csv file\n",
    "df.to_csv(r\"C:\\Users\\Vishwas Pai\\amazon1_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a223ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the flipkart and amazon reviews files to combine them together in a single csv file\n",
    "flipkart = pd.read_csv(r\"C:\\Users\\Vishwas Pai\\flipkart1_reviews.csv\")\n",
    "amazon = pd.read_csv(r\"C:\\Users\\Vishwas Pai\\amazon1_reviews.csv\")\n",
    "reviews = pd.concat([amazon, flipkart], ignore_index=True)\n",
    "reviews.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d96722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final data into csv file\n",
    "reviews.to_csv(r\"C:\\Users\\Vishwas Pai\\Review_Rating_file.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
